{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [1.0, 20.0, 300.0],\n",
    "        [3.0, 40.0, 500.0],\n",
    "        [10.0, 2.0, 4000.0],\n",
    "        [0.5, 200.0, 20.0 ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69122342, -0.57735027, -0.55772096],\n",
       "       [-0.164577  , -0.32356993, -0.43446771],\n",
       "       [ 1.67868545, -0.80575257,  1.72246417],\n",
       "       [-0.82288502,  1.70667277, -0.73027551]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Standardize the range of continuous initial variables\n",
    "# Zero-center the data (subtract the mean)\n",
    "X -= np.mean(X, axis = 0)\n",
    "# Normalization (divide by the std. deviation)\n",
    "X /= np.std(X, axis = 0)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.        , -2.30467039,  3.9494215 ],\n",
       "       [-2.30467039,  4.        , -2.17164024],\n",
       "       [ 3.9494215 , -2.17164024,  4.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Compute the covariance matrix to identify correlations\n",
    "# if positive then : the two variables increase or decrease together (correlated)\n",
    "# if negative then : One increases when the other decreases (Inversely correlated)\n",
    "cov = np.dot(X.T, X)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62164455  0.71651996  0.31647623]\n",
      " [ 0.48516229  0.03501081  0.87372295]\n",
      " [-0.61495984 -0.69668744  0.36939275]]\n",
      "[0.80880263 0.00394023 0.18725713]\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "# convert to percentages\n",
    "eig_vals /= eig_vals.sum()\n",
    "\n",
    "print(eig_vecs)\n",
    "print(eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.62164455  0.31647623]\n",
      " [ 0.48516229  0.87372295]\n",
      " [-0.61495984  0.36939275]]\n",
      "[0.80880263 0.18725713]\n"
     ]
    }
   ],
   "source": [
    "# 4. Create a feature vector to decide which principal components to keep\n",
    "num_dim_sel = 2\n",
    "threshold = np.sort(eig_vals)[-num_dim_sel]\n",
    "\n",
    "eig_vecs_selected = eig_vecs[:, eig_vals>=threshold]\n",
    "eig_vals_selected = eig_vals[eig_vals>=threshold]\n",
    "\n",
    "print(eig_vecs_selected)\n",
    "print(eig_vals_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49256268, -0.92921804],\n",
       "       [ 0.21250466, -0.4952844 ],\n",
       "       [-2.49371272,  0.4635253 ],\n",
       "       [ 1.78864537,  0.96097714]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Recast the data along the principal components axes\n",
    "X_reduced = X.dot(eig_vecs_selected)\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [\n",
    "        [1.0, 20.0, 300.0],\n",
    "        [3.0, 40.0, 500.0],\n",
    "        [10.0, 2.0, 4000.0],\n",
    "        [0.5, 200.0, 20.0 ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  903.48687339,    69.37640916],\n",
       "       [  704.08054828,    44.10716631],\n",
       "       [-2795.70947342,   -10.3163823 ],\n",
       "       [ 1188.14205175,  -103.16719317]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume input data matrix X of size [N x D]\n",
    "X -= np.mean(X, axis = 0) # zero-center the data (important)\n",
    "cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix\n",
    "\n",
    "# SVD\n",
    "U,S,V = np.linalg.svd(cov)\n",
    "\n",
    "Xrot = np.dot(X, U) # decorrelate the data\n",
    "\n",
    "# PCA\n",
    "Xrot_reduced = np.dot(X, U[:,:2]) # Xrot_reduced becomes [N x 100]\n",
    "Xrot_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.55659321,  1.04862031,  1.26117056],\n",
       "       [ 0.43374892,  0.6666772 , -1.53861351],\n",
       "       [-1.7222972 , -0.15593151,  0.09683715],\n",
       "       [ 0.73195507, -1.559366  ,  0.1806058 ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whiten the data:\n",
    "# divide by the eigenvalues (which are square roots of the singular values)\n",
    "Xwhite = Xrot / np.sqrt(S + 1e-5)\n",
    "Xwhite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important:\n",
    "\n",
    "In practice, PCA/Whitening are not used with Convolutional Networks. However, it is very important to zero-center the data, and it is common to see normalization of every pixel as well.\n",
    "\n",
    "The mean must be computed only over the training data and then subtracted equally from all splits (train/val/test)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
