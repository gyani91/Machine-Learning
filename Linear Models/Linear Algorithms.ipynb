{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt, exp\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    return sqrt(sum((row1[:-1]-row2[:-1])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the most similar neighbors\n",
    "def get_neighbors(train, test_row, num_neighbors):\n",
    "    distances = np.array(list(map(lambda x: euclidean_distance(test_row, x), train[:, :-1])))\n",
    "    enriched = np.column_stack((train, distances))\n",
    "    return enriched[enriched[:, -1].argsort()][:num_neighbors, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a classification prediction with neighbors\n",
    "def predict_classification(train, test_row, num_neighbors):\n",
    "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    "    u, c = np.unique(neighbors, return_counts = True)\n",
    "    return u[c == c.max()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    arr = np.array(dataset)\n",
    "    \n",
    "    classes, labels = str_column_to_int(arr[:, -1])\n",
    "    return np.hstack((arr[:, :-1], labels.reshape(-1, 1))).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to integer (one-hot encoding)\n",
    "def str_column_to_int(labels):\n",
    "    return np.unique(labels, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "    return np.min(dataset, axis=0), np.max(dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, min_vals, max_vals):\n",
    "    denominator = max_vals - min_vals\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i] = (dataset[i] - min_vals) / denominator\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    # random shuffle before sending \n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    return np.array_split(dataset, n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    return (actual == predicted).sum() / actual.shape[0] * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for i in range(n_folds):\n",
    "        test = folds[i]\n",
    "        train = np.concatenate([folds[j] for j in range(n_folds) if j != i])\n",
    "        \n",
    "        predicted = algorithm(train, test[:, :-1], *args)\n",
    "        actual = test[:, -1]\n",
    "        \n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Algorithm\n",
    "def k_nearest_neighbors(train, test, num_neighbors):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        output = predict_classification(train, row, num_neighbors)\n",
    "        predictions.append(output)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the kNN on the Iris Flowers dataset\n",
    "filename = 'data/iris.csv'\n",
    "dataset = load_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "min_vals, max_vals = dataset_minmax(dataset[:, :-1])\n",
    "dataset[:, :-1] = normalize_dataset(dataset[:, :-1], min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [93.33333333333333, 90.0, 96.66666666666667, 86.66666666666667, 96.66666666666667]\n",
      "Mean Accuracy: 92.667%\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "num_neighbors = 5\n",
    "\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return np.mean(values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance of a list of numbers\n",
    "def variance(values, mean):\n",
    "    return sum((values - mean)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance between x and y\n",
    "def covariance(x, mean_x, y, mean_y):\n",
    "    return sum((x - mean_x) * (y - mean_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficients\n",
    "def coefficients(dataset):\n",
    "    x_mean, y_mean = mean(dataset)\n",
    "    x_variance = variance(dataset[:, 0], x_mean)\n",
    "    \n",
    "    b1 = covariance(dataset[:, 0], x_mean, dataset[:, 1], y_mean) / x_variance\n",
    "    b0 = y_mean - b1 * x_mean\n",
    "    \n",
    "    return [b0, b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simple linear regression\n",
    "dataset = np.array([[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]])\n",
    "mean_ds = mean(dataset)\n",
    "variance_ds = variance(dataset, mean_ds)\n",
    "covariance_ds = covariance(dataset[:, 0], mean_ds[0], dataset[:, 1], mean_ds[1])\n",
    "coeff = coefficients(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a txt file\n",
    "def load_txt(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            dataset.append(line.split())\n",
    "    return np.array(dataset, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, split):\n",
    "    training_elements = int(split*dataset.shape[0])\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset[:training_elements], dataset[training_elements:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_metric(actual, predicted):\n",
    "    return np.sqrt(np.mean((predicted-actual)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a train/test split\n",
    "def evaluate_algorithm(dataset, algorithm, split, *args):\n",
    "    train, test = train_test_split(dataset, split)\n",
    "    \n",
    "    predicted = algorithm(train, test[:, :-1], *args)\n",
    "    actual = test[:, -1]\n",
    "    \n",
    "    rmse = rmse_metric(actual, predicted)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(train, test):\n",
    "    b0, b1 = coefficients(train)\n",
    "    return test*b1 + b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression on insurance dataset\n",
    "# load and prepare data\n",
    "filename = 'data/AutoInsurSweden.txt'\n",
    "dataset = load_txt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 42.319\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "split = 0.9\n",
    "rmse = evaluate_algorithm(dataset, simple_linear_regression, split)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    return sum(row * coefficients[:-1]) + coefficients[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = np.zeros(train.shape[1])\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            \n",
    "            yhat = predict(row[:-1], coef)\n",
    "            \n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            \n",
    "            # update weights\n",
    "            coef[:-1] -= (row[:-1] * (l_rate * error))\n",
    "            \n",
    "            # update bias\n",
    "            coef[-1] -= l_rate * error\n",
    "            \n",
    "#         print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        for row in file.readlines():\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row.split(\";\"))\n",
    "    return np.array(dataset[1:], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm With Stochastic Gradient Descent\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    \n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    \n",
    "    for row in test:\n",
    "        yhat = predict(row[:-1], coef)\n",
    "        predictions.append(yhat)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "filename = 'data/winequality-white.csv'\n",
    "dataset = load_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "min_vals, max_vals = dataset_minmax(dataset[:, :-1])\n",
    "dataset[:, :-1] = normalize_dataset(dataset[:, :-1], min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, metric, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for i in range(n_folds):\n",
    "        test = folds[i]\n",
    "        train = np.concatenate([folds[j] for j in range(n_folds) if j != i])\n",
    "        \n",
    "        predicted = algorithm(train, test, *args)\n",
    "        actual = test[:, -1]\n",
    "        \n",
    "        result = metric(actual, predicted)\n",
    "        scores.append(result)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.7618192718728922, 0.7398903864832463, 0.7558842278594348, 0.7669434343985889, 0.7636128414004474]\n",
      "Mean RMSE: 0.758\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, rmse_metric, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80172203 0.22998235]\n"
     ]
    }
   ],
   "source": [
    "dataset = np.array([[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]])\n",
    "coef = np.array([0.8, 0.4])\n",
    "l_rate = 0.001\n",
    "n_epoch = 50\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = sum(row * coefficients[:-1]) + coefficients[-1]\n",
    "    return 1.0 / (1.0 + exp(-yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = np.zeros(train.shape[1])\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row[:-1], coef)\n",
    "            \n",
    "            error = row[-1] - yhat\n",
    "            \n",
    "            # update weights\n",
    "            coef[:-1] += (row[:-1] * (l_rate * error * yhat * (1.0 - yhat)))\n",
    "            \n",
    "            # update bias\n",
    "            coef[-1] += l_rate * error * yhat * (1.0 - yhat)\n",
    "\n",
    "#         print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Algorithm With Stochastic Gradient Descent\n",
    "def logistic_regression(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    \n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    \n",
    "    for row in test:\n",
    "        yhat = predict(row[:-1], coef)\n",
    "        predictions.append(round(yhat))\n",
    "    \n",
    "    return np.array(predictions, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "        return np.array(dataset, dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "filename = 'data/pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "min_vals, max_vals = dataset_minmax(dataset[:, :-1])\n",
    "dataset[:, :-1] = normalize_dataset(dataset[:, :-1], min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [72.72727272727273, 81.81818181818183, 74.67532467532467, 73.20261437908496, 73.8562091503268]\n",
      "Mean Accuracy: 75.256%\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.1\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(dataset, logistic_regression, accuracy_metric, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "    activation = sum(row * weights[:-1]) + weights[-1]\n",
    "    return 1.0 if activation >= 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    weights = train_weights(train, l_rate, n_epoch)\n",
    "    \n",
    "    for row in test:\n",
    "        prediction = predict(row[:-1], weights)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "    weights = np.zeros(train.shape[1])\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0.0\n",
    "        for row in train:\n",
    "            prediction = predict(row[:-1], weights)\n",
    "            error = row[-1] - prediction\n",
    "            sum_error += error**2\n",
    "            \n",
    "            # update weights\n",
    "            weights[:-1] += (row[:-1] * (l_rate * error))\n",
    "            \n",
    "            # update bias\n",
    "            weights[-1] += l_rate * error\n",
    "            \n",
    "#         print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    arr = np.array(dataset)\n",
    "    \n",
    "    classes, labels = str_column_to_int(arr[:, -1])\n",
    "    return np.hstack((arr[:, :-1], labels.reshape(-1, 1))).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/sonar.csv'\n",
    "dataset = load_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [64.28571428571429, 68.11594202898551, 78.26086956521739]\n",
      "Mean Accuracy: 70.221%\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, accuracy_metric, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jup)",
   "language": "python",
   "name": "jup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
